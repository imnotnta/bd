{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!apt-get update -y\n","!apt-get install -y \\\n","libglib2.0-0 \\\n","libnss3 \\\n","libdbus-glib-1-2 \\\n","libgconf-2-4 \\\n","libfontconfig1 \\\n","libvulkan1 \\\n","gconf2-common \\\n","libwayland-server0 \\\n","libgbm1 \\\n","udev \\\n","libu2f-udev \n","!apt --fix-broken install -y  \n","!wget -P /tmp https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/linux64/chrome-linux64.zip\n","!unzip /tmp/chrome-linux64.zip -d /usr/bin/\n","!wget -P /tmp https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/linux64/chromedriver-linux64.zip\n","!unzip /tmp/chromedriver-linux64.zip -d /usr/bin/\n","!apt install -y python3-selenium\n","!pip install selenium==4.7.0\n","!pip install kafka-python webdriver_manager\n","!pip install -q gdown\n","\n","import time, random\n","from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","from selenium.common.exceptions import TimeoutException\n","from selenium.common.exceptions import NoSuchElementException\n","from selenium.webdriver.common.action_chains import ActionChains\n","from selenium.webdriver.common.keys import Keys\n","\n","from bs4 import BeautifulSoup\n","from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","\n","from webdriver_manager.chrome import ChromeDriverManager\n","import pickle\n","from kafka import KafkaProducer\n","import json \n","import os\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!git clone https://github.com/imnotnta/bd.git\n","%cd /kaggle/working/bd\n","!git checkout Huy\n","%cd /kaggle/working/bd"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["CHROME_BINARY_LOCATION = \"/usr/bin/chrome-linux64/chrome\"\n","CHROMEDRIVER_BINARY_LOCATION = \"/usr/bin/chromedriver-linux64/chromedriver\"\n","\n","def add_driver_options(options):\n","    \"\"\"\n","    Add configurable options\n","    \"\"\"\n","    chrome_options = Options()\n","    for opt in options:\n","        chrome_options.add_argument(opt)\n","    return chrome_options\n","\n","def initialize_driver():\n","    \"\"\"\n","    Initialize the web driver\n","    \"\"\"\n","    driver_config = {\n","        \"options\": [\n","            \"--headless\",\n","            \"--no-sandbox\",\n","            \"--allow-insecure-localhost\",\n","            \"--disable-dev-shm-usage\",\n","            \"--incognito\",\n","            \"--window-size=1920x1080\"\n","            \"user-agent=Chrome/116.0.5845.96\"\n","        ],\n","    }\n","    options = add_driver_options(driver_config[\"options\"])\n","    options.binary_location = CHROME_BINARY_LOCATION\n","    driver = webdriver.Chrome(\n","        executable_path=CHROMEDRIVER_BINARY_LOCATION,\n","        options=options)\n","    return driver\n","driver = initialize_driver()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["username = '____'\n","password = '____'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["driver.get(\"https://twitter.com/\")\n","time.sleep(3)\n","driver.find_element(\"xpath\",\"//a[@href='/login']\").click()\n","time.sleep(3)\n","driver.find_element(\"xpath\",\"//label\").click()\n","time.sleep(2)\n","driver.find_element(\"xpath\",\"//label\").send_keys(username)\n","time.sleep(2)\n","driver.find_element(\"xpath\",\"//label\").send_keys(Keys.ENTER)\n","time.sleep(2)\n","driver.find_elements(\"xpath\",'//input')[1].send_keys(password)\n","driver.find_elements(\"xpath\",'//input')[1].send_keys(Keys.ENTER)\n","\n","print(\"Finish enter password, you have 2 minutes\")\n","time.sleep(60 * 2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!gdown ________________________"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from PIL import Image"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# driver.get(f\"https://twitter.com/cryptoueen999\") #These posts are protected\n","driver.save_screenshot(f'logged-in-1.png')\n","# Image.open('logged-in-1.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["key_path = f\"/kaggle/working/bd/key_file.txt\"\n","with open(key_path, 'r') as f:\n","    line = f.readlines()[0].strip().lower()\n","    print(f\"Key = {line}\")\n","import os\n","os.remove(key_path)\n","if line != 'n':\n","    driver.find_elements(\"xpath\",\"//input\")[0].send_keys(line)\n","    driver.find_elements(\"xpath\",\"//input\")[0].send_keys(Keys.ENTER)\n","print(\"Đã nhập key xong, chờ 5s để vào trang đăng nhập\")\n","time.sleep(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["driver.save_screenshot(f'logged-in-2.png')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if \"Subscribe to unlock new features and if eligible, receive a share of ads revenue\" in driver.page_source:\n","    language = \"English\"\n","elif \"Đăng ký để mở khóa các tính năng mới và nếu đủ điều kiện, bạn sẽ được nhận một khoản chia sẻ doanh thu từ quảng cáo\" in driver.page_source:\n","    language = \"Vietnamese\"\n","else:\n","    assert False, \"Không phải Tiếng Anh hay Tiếng Việt\"\n","print(language)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class TwitterCrawler:\n","    def __init__(self, producer: KafkaProducer = None, hashtags: list = None, driver = driver):\n","        self.driver = driver\n","        self.hashtags = hashtags\n","        self.producer = producer\n","        self.groups = []\n","        self.save_file = f\"./crawled_data\" \n","        os.makedirs(f\"{self.save_file}/all_users\", exist_ok=True)\n","        os.makedirs(f\"{self.save_file}/groups\", exist_ok=True)\n","        os.makedirs(f\"{self.save_file}/user_info\", exist_ok=True)\n","    \n","    def extract_to_dictionary(self,text):\n","        data_dict = {'replies': 0, 'reposts': 0, 'likes': 0, 'views': 0, 'bookmarks': 0}\n","        data = text.split(',')\n","        for item in data:\n","            item = item.strip()\n","            if 'repl' in item:\n","                data_dict['replies'] = int(item.split()[0])\n","            elif 'repost' in item:\n","                data_dict['reposts'] = int(item.split()[0])\n","            elif 'like' in item:\n","                data_dict['likes'] = int(item.split()[0])\n","            elif 'bookmark' in item:\n","                data_dict['bookmarks'] = int(item.split()[0])\n","\n","            elif 'views' in item:\n","                data_dict['views'] = int(item.split()[0])\n","\n","        return data_dict\n","    \n","    def retrieve_tweets(self, username):\n","        count = 0\n","        iter = 1\n","        tweet_dict = {}\n","        while iter < 6 and count < 20:\n","            tweets = self.driver.find_elements(\"xpath\",\"//article[@tabindex='0']\")\n","            for i in range(len(tweets)):\n","                tweet_time = tweets[i].find_element(\"xpath\",\".//time[@datetime and @datetime!='none']\")\n","                tweet_time = tweet_time.get_attribute('datetime') + ''\n","                if tweet_time in tweet_dict.keys():\n","                    continue\n","                else:\n","                    if tweets[i].find_elements(\"xpath\",f'.//a[@href=\"/{username}\" and contains(@dir,\"ltr\")]') != []:\n","                        repost = \"True\" \n","                    else:\n","                        repost = \"False\"\n","                    try:    \n","                        interacts = tweets[i].find_elements(\"xpath\",f'.//div[@role=\"group\"]')[0].get_attribute('aria-label')\n","\n","                        interacts = self.extract_to_dictionary(interacts)\n","                    except:\n","                        interacts = {'replies': 0, 'reposts': 0, 'likes': 0, 'views': 0, 'bookmarks': 0}\n","                    try:\n","                        langs = tweets[i].find_element(\"xpath\", f'.//div[@lang and @lang!=\"none\"]')\n","                        try:\n","                            tweet_content = ' '.join([line.text for line in langs.find_elements(\"xpath\",f\".//*\")])\n","                        except:\n","                            tweet_content = 'Video'\n","                    except:\n","                        langs = \"NA\"\n","                        tweet_content = 'Video'\n","\n","                    tweet_dict.update({tweet_time: {'repost': repost,'content': tweet_content, 'language': langs.get_attribute('lang') if langs != \"NA\" else \"NA\",\n","                                                    'interacts': interacts}})\n","                    count += 1\n","                    if count == 20:\n","                        break\n","            #Scroll down\n","            self.driver.execute_script(f\"window.scrollTo(0,{3000*iter})\")\n","            time.sleep(1)\n","            iter+= 1\n","        \n","        return tweet_dict\n","    \n","    def retrieve_basic_user_info(self,username):\n","        self.driver.get(f'https://twitter.com/{username}')\n","        time.sleep(2)\n","        \n","        page_src = self.driver.page_source\n","        if language == 'Vietnamese':\n","            if \"Đã xảy ra lỗi. Thử tải lại\" in page_src:\n","                return \"Something went wrong. Try reloading.\"\n","#             elif \"Các bài đăng này được bảo vệ\" in page_src:\n","#                 return \"These posts are protected\"\n","        elif language == 'English':\n","            if \"Something went wrong. Try reloading.\" in page_src:\n","                return \"Something went wrong. Try reloading.\"\n","#             elif \"These posts are protected\" in page_src:\n","#                 return \"These posts are protected\"\n","    \n","        user_info = self.driver.find_elements(\"xpath\",f'//script[@type=\"application/ld+json\"]')\n","        jsontext = json.loads(user_info[0].get_attribute('innerHTML'))\n","        try:\n","            job = self.driver.find_element(\"xpath\", \"//span[@data-testid ='UserProfessionalCategory']/*[2]/*\").text\n","        except:\n","            job = \"NA\"\n","            \n","        verified = self.driver.find_elements(\"xpath\", '//div[@aria-label =\"Provides details about verified accounts.\"]/*/*/*')\n","        if len(verified) == 0:\n","            type_account = 'normal'\n","        elif len(verified) == 1:\n","            if verified[0].get_attribute('clip-rule') == 'evenodd':\n","                type_account = 'government'\n","            else:\n","                type_account = 'verified'\n","        else:\n","            type_account = 'company'\n","        if len(self.driver.find_elements(\"xpath\",'//div[@aria-label=\"Provides details about protected accounts.\"]')) > 0:\n","            tweets = {}\n","            protected = 'True' \n","        else:\n","            protected = 'False'\n","            if len(self.driver.find_elements(\"xpath\",'//div[@data-testid=\"emptyState\"]')) > 0:\n","                tweets = {}\n","            else: \n","                tweets = self.retrieve_tweets(username)\n","                \n","\n","        return_dict = {'type':jsontext['author']['@type'],\n","                        'name':jsontext['author']['givenName'],\n","                    'location':jsontext['author']['homeLocation']['name'],\n","                    'description':jsontext['author']['description'],\n","                    'date_created': jsontext['dateCreated'],\n","                    'num_followers': jsontext['author']['interactionStatistic'][0]['userInteractionCount'],\n","                    'num_following': jsontext['author']['interactionStatistic'][1]['userInteractionCount'],\n","                    'num_tweets': jsontext['author']['interactionStatistic'][2]['userInteractionCount'],\n","                    'job': job, 'type_account': type_account, 'protected': protected, 'all_tweets': tweets}\n","        return return_dict\n","    \n","    def load_username(self, keyword):\n","        try:\n","            with open(f\"{self.save_file}/all_users/{keyword}.pkl\", \"rb\") as f:\n","                list_users = pickle.load(f)\n","            with open(f\"{self.save_file}/user_info/info.pkl\", \"rb\") as f:\n","                dct = pickle.load(f)\n","            users_left = list(set(list_users) - set(dct['Finished']) - set(dct['Error']))\n","            return users_left\n","        except:\n","            return []\n","        \n","    def load_crawled_username(self, keyword):\n","        return [x.split(\".\")[0] for x in os.listdir(f\"{self.save_file}/user_info/{keyword}\")]\n","\n","    def write_user_info_to_file(self, keyword, user_info, user_name):\n","        with open(f\"{self.save_file}/user_info/{keyword}/{user_name}.pkl\", \"wb\") as f:\n","            pickle.dump(user_info, f)\n","            \n","    def write_user_info_to_kafka(self, keyword, user_info, user_name):\n","        self.producer.send(f\"{keyword}_{user_name}\", pickle.dumps(user_info))\n","    \n","    def crawl_all_users(self, hashtag, reverse=False):\n","        user_names = self.load_username(hashtag)\n","        os.makedirs(f\"{self.save_file}/user_info/{hashtag}\", exist_ok=True)\n","        file_log_he = open(\"/kaggle/working/logs.txt\", 'w')\n","        crawled_users = self.load_crawled_username(hashtag)\n","        user_names = sorted(list(set(user_names) - set(crawled_users))) #Make sure that we don't crawl one file we have already crawled\n","        print(f\"Num user left: {len(user_names)}\")\n","\n","        assert reverse in [False, True]\n","        start, end, step = (len(user_names) - 1, -1, -1) if reverse else (0, len(user_names), 1)\n","\n","        for idx in range(start, end, step):\n","\n","            try:\n","                user_info = self.retrieve_basic_user_info(user_names[idx])\n","            except:\n","                file_log_he.write(f\"Error when crawling user: {user_names[idx]}\\n\")\n","                print(\"Error when crawling user: \", user_names[idx])\n","                time.sleep(random.randint(20, 40))\n","                continue\n","            \n","            if isinstance(user_info, str):\n","                print('Skip crawling user: ', user_names[idx])\n","                file_log_he.write(f\"Skip crawling user: {user_names[idx]}\\n\")\n","            else:\n","                self.write_user_info_to_file(hashtag, user_info, user_names[idx])\n","                # self.write_user_info_to_kafka(hashtag, user_info, user_names[idx])\n","                print('Finish crawling user: ', user_names[idx])\n","                file_log_he.write(f\"Finish crawling user: {user_names[idx]}\\n\")\n","                \n","            time.sleep(random.randint(20, 40))\n","\n","        print('Finish crawling users from hashtag: ', hashtag)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# producer = KafkaProducer(bootstrap_servers=['34.142.194.212:29092'])\n","producer = None\n","crawler = TwitterCrawler(producer, ['Bitcoin'], driver = driver)\n","crawler.crawl_all_users('Bitcoin', reverse=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"}},"nbformat":4,"nbformat_minor":4}
